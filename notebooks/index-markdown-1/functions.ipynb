{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4af151-7e66-4de6-a7d9-a3252b6c72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_md(file_path)\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "def split_md(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_document = file.read()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\")\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "    chunk_size = 350\n",
    "    chunk_overlap = 80\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    results = []\n",
    "    for split in splits:\n",
    "        # Calculate content with headers\n",
    "        headers_ = '\\n'.join([f'{\"#\"*int(k[-1])} {v}' for k, v in split.metadata.items() if 'Header' in k])\n",
    "        content_ = f\"{headers_}\\n\\n{split.page_content}\"\n",
    "    \n",
    "        # Add an item into the array\n",
    "        results.append({\n",
    "            \"hash\": hash(content_),\n",
    "            \"content\": content_,\n",
    "            \"metadata\": json.dumps(split.metadata),\n",
    "            \"document\": split,\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a741f28-fd9d-4f3c-ba06-51c17895e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_markdown(reference, vector_size = 1500):\n",
    "    # Tokenizes the input document\n",
    "    splits = split_md(reference)\n",
    "    splits_hashes = [item['hash'] for item in splits];\n",
    "\n",
    "    # Remove lines that have changed\n",
    "    execute_query(\"\"\"\n",
    "    DELETE FROM mkd01 \n",
    "    WHERE reference = %s \n",
    "    AND hash NOT IN (SELECT unnest(%s::text[]))\n",
    "    \"\"\", (reference, splits_hashes))\n",
    "\n",
    "    # Finds hashes that are not present in the table\n",
    "    existing_hashes = [hash_[0] for hash_ in execute_query(\n",
    "        \"SELECT hash FROM mkd01 WHERE reference = %s\",\n",
    "        (reference,)\n",
    "    )]\n",
    "    \n",
    "    filtered_hashes = [str(item) for item in splits_hashes if str(item) not in existing_hashes]\n",
    "    matching_splits = [item for item in splits if str(item['hash']) in filtered_hashes]\n",
    "\n",
    "    # Adds & Embeds lines that has changed\n",
    "    #import json\n",
    "    \n",
    "    query = \"\"\"\n",
    "    INSERT INTO mkd01\n",
    "           (hash, reference, index, content, metadata, embedding)\n",
    "    VALUES (%s,   %s,        %s,    %s,      %s,       %s)\n",
    "    ON CONFLICT DO NOTHING;\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, split in enumerate(matching_splits):\n",
    "        print(f\"embed: {reference}\")\n",
    "        embeddings = generate_embedding(split[\"content\"], vector_size)\n",
    "        execute_query(query, (\n",
    "            split[\"hash\"],\n",
    "            reference,\n",
    "            index,\n",
    "            split[\"content\"], \n",
    "            split[\"metadata\"],\n",
    "            embeddings\n",
    "        ))\n",
    "\n",
    "    return matching_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f807859-3838-44d0-b4e2-9923f75cd878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def find_sources(base_path, filter_as='*.md'):\n",
    "    markdown_files = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Skip hidden directories by modifying the dirs list in-place\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        # Filter files in the current root folder based on the provided pattern\n",
    "        for file in fnmatch.filter(files, filter_as):\n",
    "            # Skip files that start with a dot\n",
    "            if file.startswith('.'):\n",
    "                continue\n",
    "            # Construct the relative path from the base path\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), start=base_path)\n",
    "            # Append the relative path prefixed with ./ for local path context\n",
    "            markdown_files.append(os.path.join(base_path, relative_path))\n",
    "    return markdown_files\n",
    "\n",
    "#find_sources('./docs-onefront-small/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5d4f4-853d-4df5-b8d3-0926e1564c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
